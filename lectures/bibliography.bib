%
% Main textbooks
%

@book{Aggarwal:2018SpringerDL,
    author = "{Charu C. Aggarwal}",
    title = "{Neural Network and Deep Learning}",
    year = "2018",
    publisher="Springer",
    address = "",
    doi = "10.1007/978-3-319-94463-0",
}

@book{Goodfellow:2017MITDL,
    author = "{Ian Goodfellow, Yoshua Bengio and Aaron Courville}",
    title = "{Deep Learning}",
    year = "2017",
    publisher = "MIT Press",
    isbn = "9780262035613",
    note = "Series: Adaptive computation and machine learning series"
}

@book{Watt:2016Cambridge, 
    author={Watt, Jeremy and Borhani, Reza and Katsaggelos, Aggelos K.}, 
    title={Machine Learning Refined: Foundations, Algorithms, and Applications}, 
    year={2016},
    publisher={Cambridge University Press}, 
    doi={10.1017/CBO9781316402276}, 
}

@book{Montavon:2012TricksOfTrade,
  editor       = {Gr{\'{e}}goire Montavon and
                  Genevieve B. Orr and
                  Klaus{-}Robert M{\"{u}}ller},
  title        = {Neural Networks: Tricks of the Trade - Second Edition},
  series       = {Lecture Notes in Computer Science},
  volume       = {7700},
  publisher    = {Springer},
  year         = {2012},
  url          = {https://doi.org/10.1007/978-3-642-35289-8},
  doi          = {10.1007/978-3-642-35289-8},
  isbn         = {978-3-642-35288-1},
  timestamp    = {Wed, 25 Sep 2019 17:41:07 +0200},
  biburl       = {https://dblp.org/rec/series/lncs/7700.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

%
% Other general resources
%

@online{MatrixCookbook,
  title={The Matrix Cookbook.}, 
  author = {Kaare Brandt Petersen and Michael Syskind Pedersen},
  url = {https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf},  
  urldate = {Accessed: 2023-11-22}
}

@online{CS231n,
  title={Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.}, 
  url = {https://cs231n.github.io/},  
  urldate = {Accessed: 2023-04-20}
}

%
% AI and intelligence 
%

% What is AI
@misc {McCarthy:2007ai,
  author={J. McCarthy},
  title={What is Artificial Intelligence}, 
  year={2007},
  url = {https://www-formal.stanford.edu/jmc/whatisai.pdf},  
  urldate = {2023-04-20}
}

% Intelligence
@article {Neisser:1996intl,
  author={U. Neisser and G. Boodoo and T.J. Bouchard and A.W. Boykin and N. Brody and S.J. Ceci 
   and D.F. Halpern and J.C. Loehlin and R. Perloff and R.J. Sternberg and S. Urbina},
  journal = {American Psychologist},
  pages = {77--101},
  title = {Intelligence: Knowns and unknowns},
  volume = {51},
  number = {2},
  year = {1996},
  doi = {https://doi.org/10.1037/0003-066X.51.2.77}
}

%
% Biological inspirations
%

% Electrical activity of the brain
@article {Caton:1875,
	editor = {,},
	title = {Forty-Third Annual Meeting of the British Medical Association},
	volume = {2},
	number = {765},
	pages = {257--279},
	year = {1875},
	doi = {10.1136/bmj.2.765.257},
	publisher = {BMJ Publishing Group Ltd},
	issn = {0007-1447},
	URL = {https://www.bmj.com/content/2/765/257},
	eprint = {https://www.bmj.com/content/2/765/257.full.pdf},
	journal = {BMJ}
}
@article {Berger:1929,
	author = {H. Berger},
	title = {Ueber das Elektroenkephalogramm des Menschen},
	volume = {87},
	number = {1},
	pages = {527--570},
	year = {1929},
	doi = {10.1007/BF01797193},
	URL = {https://hdl.handle.net/11858/00-001M-0000-002A-5DE0-7},
	journal = {Archiv f{\"u}r Psychiatrie und Nervenkrankheiten}
}

% First studies of the visual cortex (V1) motivating CNNs
@article{Hubel:1959v1,
  author = {Hubel, D. and Wiesel, T.},
  journal = {Journal of Physiology},
  pages = {574--591},
  title = {Receptive fields of single neurones in the cat's striate cortex},
  volume = 148,
  year = 1959,
  doi = {10.1113/jphysiol.1959.sp006308}
}
@article{Hubel:1962v1,
  author = {Hubel, D. and Wiesel, T.},
  journal = {Journal of Physiology},
  pages = {106--154},
  title = {Receptive fields, binocular interaction, and functional 
    architecture in the cat's visual cortex},
  volume = 160,
  year = 1962,
  doi = {10.1113/jphysiol.1962.sp006837}
}

%
% Artificial halfwits
%
@online{GoogDoc:GamingExamplesinAI,
  title={Specification gaming examples in AI}, 
  url = {https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml},  
  urldate = {Accessed: 2023-04-20}
}
@misc{Lehman:2019Surprising,
  author={Joel Lehman and Jeff Clune and Dusan Misevic and Christoph Adami and Lee Altenberg and Julie Beaulieu and Peter J. Bentley and Samuel Bernard and Guillaume Beslon and David M. Bryson and Patryk Chrabaszcz and Nick Cheney and Antoine Cully and Stephane Doncieux and Fred C. Dyer and Kai Olav Ellefsen and Robert Feldt and Stephan Fischer and Stephanie Forrest and Antoine Frénoy and Christian Gagné and Leni Le Goff and Laura M. Grabowski and Babak Hodjat and Frank Hutter and Laurent Keller and Carole Knibbe and Peter Krcah and Richard E. Lenski and Hod Lipson and Robert MacCurdy and Carlos Maestre and Risto Miikkulainen and Sara Mitri and David E. Moriarty and Jean-Baptiste Mouret and Anh Nguyen and Charles Ofria and Marc Parizeau and David Parsons and Robert T. Pennock and William F. Punch and Thomas S. Ray and Marc Schoenauer and Eric Shulte and Karl Sims and Kenneth O. Stanley and François Taddei and Danesh Tarapore and Simon Thibault and Westley Weimer and Richard Watson and Jason Yosinski},
  title={The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities}, 
  year={2019},
  eprint={1803.03453},
  archivePrefix={arXiv},
  primaryClass={cs.NE}
}

%
% Surveys of Artificial General Intelligence R&D projects
%
@online{GCRI:2020agi,
  author={M. Fitzgerald and A. Boddy and S.D. Baum},
  title={2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy}, 
  year = {2020},
  url = {https://gcrinstitute.org/papers/055_agi-2020.pdf},  
  urldate = {Accessed: 2023-04-20}
}

% Turing test
@article{Turing:1950tt,
    author = {Turing, A. M.},
    title = "{I. — Computing Machinery and Intelligence}",
    journal = {Mind},
    volume = {LIX},
    number = {236},
    pages = {433-460},
    year = {1950},
    month = {10},
    issn = {0026-4423},
    doi = {10.1093/mind/LIX.236.433},
    url = {https://doi.org/10.1093/mind/LIX.236.433},
    eprint = {https://academic.oup.com/mind/article-pdf/LIX/236/433/30123314/lix-236-433.pdf},
}

% Information theory
@article {Shannon:1948,
	author = {C.E. Shannon},
	title = {A Mathematical Theory of Communication},
	volume = {27},
	year = {1948},
	URL = {https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf},
	journal = {The Bell System Technical Journal}
}

% Dartmouth conference (1956) proposal
@article{Dartmouth:1956, 
    author={McCarthy, John and Minsky, Marvin L. and Rochester, Nathaniel and Shannon, Claude E.}, 
    title={A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955}, 
    volume={27}, 
    number={4}, 
    journal={AI Magazine}, 
    year={2006},
    doi={10.1609/aimag.v27i4.1904}, 
    url={https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1904} 
}
% Dartmouth conference 
@online{Veisdal:2019dartmouth,
  author = {Jorgen Veisdal},
  title = {The Birthplace of AI - The 1956 Dartmouth Workshop},
  year = {2019},
  url = {https://www.cantorsparadise.com/the-birthplace-of-ai-9ab7d4e5fb00},  
  urldate = {2023-04-21}
}

% Logical theorist
@online{Newell:1956logth,
  author = {A.Newell and H.Simon},
  title = {The Logic Theory Machine: A Complex Information Processing System},
  year = {1956},
  url = {http://shelf1.library.cmu.edu/IMLS/MindModels/logictheorymachine.pdf},  
  urldate = {2023-04-21}
}

% Various general references on AI history
@book{Crevier:1993,
    author = "{Daniel Crevier}",
    title = "{The tumultuous history of the search for Artificial Intelligence}",
    year = "1996",
    publisher="BasicBooks, A Division of HarperCollins Publishers, Inc.",
    isbn = "0-465-02997-3",
    note = {\url{https://www.researchgate.net/publication/233820788_AI_The_Tumultuous_History_of_the_Search_for_Artificial_Intelligence}}
}

%
% Gradient descent (GD) and practical recommendations
%

% Primary GD reference
@article{Cauchy:GradientDescent,
  author="CAUCHY, A.",
  title="Methode generale pour la resolution des systemes d'equations simultanees",
  journal="C.R. Acad. Sci. Paris",
  year="1847",
  volume="25",
  pages="536-538",
  URL="https://cir.nii.ac.jp/crid/1573387450834953216"
}

% Practical recommendations
@Inbook{Bengio:2012gbt,
  author="Bengio, Yoshua",
  editor="Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert",
  title="Practical Recommendations for Gradient-Based Training of Deep Architectures",
  bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
  year="2012",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="437--478",
  isbn="978-3-642-35289-8",
  doi="10.1007/978-3-642-35289-8_26",
  url="https://doi.org/10.1007/978-3-642-35289-8_26"
}

% Learning rate decay
@misc{You:2019lrd,
  author={Kaichao You and Mingsheng Long and Jianmin Wang and Michael I. Jordan},
  title={How Does Learning Rate Decay Help Modern Neural Networks?}, 
  year={2019},
  eprint={1908.01878},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

% Escaping local minima
@misc{Kleinberg:2018gdlm,
  author={Robert Kleinberg and Yuanzhi Li and Yang Yuan},
  title={An Alternative View: When Does SGD Escape Local Minima?}, 
  year={2018},
  eprint={1802.06175},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

% Momentum based methods, primary reference
@article{Polyak:1964a,
  author = {B.T. Polyak},
  title = {Some methods of speeding up the convergence of iteration methods},
  journal = {USSR Computational Mathematics and Mathematical Physics},
  volume = {4},
  number = {5},
  pages = {1-17},
  year = {1964},
  issn = {0041-5553},
  doi = {https://doi.org/10.1016/0041-5553(64)90137-5},
  url = {https://www.sciencedirect.com/science/article/pii/0041555364901375}
}

% Nesterov momentum
@article{Nesterov:1983a,
  title={A method for solving the convex programming problem with convergence rate O(1/k$^2$)},
  author={Yurii Nesterov},
  journal={Proceedings of the USSR Academy of Sciences},
  year={1983},
  volume={269},
  pages={543-547},
  url={https://api.semanticscholar.org/CorpusID:145918791}
}

% Delta-bar-delta
@article{Jacobs:1988dbd,
  author = {Robert A. Jacobs},
  title = {Increased rates of convergence through learning rate adaptation},
  journal = {Neural Networks},
  volume = {1},
  number = {4},
  pages = {295-307},
  year = {1988},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/0893-6080(88)90003-2},
  url = {https://www.sciencedirect.com/science/article/pii/0893608088900032}
}

% AdaGrad sub-gradient method
@article{Duchi:11a,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121--2159},
  url     = {http://jmlr.org/papers/v12/duchi11a.html}
}

% RMSProp
@misc {Hinton:2012rmsp,
  author={G. Hinton and T. Tieleman},
  title={Divide the Gradient by a Running Average of Its Recent Magnitude,
  COURSERA: Neural Networks for Machine Learning, 4, 26-31}, 
  year={2012},
  url = {}  
}

% Adam
@misc{Kingma:2017adam,
  title={Adam: A Method for Stochastic Optimization}, 
  author={Diederik P. Kingma and Jimmy Ba},
  year={2017},
  eprint={1412.6980},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

% Automatic differentiation
@incollection{Bucker:2005ABo,
     author = "H. M. B{\"u}cker and G. F. Corliss",
     title = "A Bibliography on Automatic Differentiation",
     booktitle = "Automatic Differentiation: {A}pplications, Theory, and Implementations",
     editor = "H. M. B{\"u}cker and G. F. Corliss and P. D. Hovland and U. Naumann and B. Norris",
     series = "Lecture Notes in Computational Science and Engineering",
     volume = "50",
     address = "New York, NY",
     year = "2005",
     publisher = "Springer",
     pages = "321--322",
     doi = "10.1007/3-540-28438-9_28"
} 
@article{Margossian:2019ad,
	  author = {Charles C. Margossian},
	  title = {A review of automatic differentiation and its efficient implementation},
	  doi = {10.1002/widm.1305},
	  url = {https://doi.org/10.1002%2Fwidm.1305},
	  year = 2019,
	  month = {mar},
	  publisher = {Wiley},
	  volume = {9},
	  number = {4},
	  journal = {{WIREs} Data Mining and Knowledge Discovery}
}
% original back-propagation algorithm
@article{Rumelhart:1986bp,
    author = "David E. Rumelhart, Geoffrey E. Hinton and Ronald J. Williams",
    title = "{Learning representations by back-propagating errors}",
    doi = "10.1038/323533a0",
    url = {https://doi.org/10.1038/323533a0},
    journal = "Nature",
    volume = "323",
    pages = {533--536},
    year = "1986"
}

% Original perceptron
@article{McCulloch:1943p,
    author = "Warren S.McCulloch and Walter Pitts",
    title = "{A logical calculus of the ideas immanent in nervous activity}",
    doi = "10.1007/BF02478259",
    journal = "The bulletin of mathematical biophysics",
    volume = "5",
    number = "4",
    year = "1943"
}
@article{Rosenblatt:1958p,
    author = "Frank Rosenlatt",
    title = "{The perceptron: A probabilistic model for information storage and organization in the brain}",
    doi = "10.1037/h0042519",
    journal = "Psychological Review",
    volume = "65",
    number = "6",
    year = "1958"
}

% Widrow Hoff
@inproceedings{Widrow:1960as,
  author = {Widrow, Bernard and Hoff, Marcian E.},
  biburl = {https://www.bibsonomy.org/bibtex/24c3b6ae932deb6bb1d04ad76c9c94a69/schaul},
  booktitle = {1960 {IRE} {WESCON} Convention Record, Part 4},
  address = {New York},
  pages = {96--104},
  publisher = {{IRE}},
  title = {Adaptive Switching Circuits},
  year = 1960
}

% Neocognitron
@article{Fukushima:1980nc,
  author = {Fukushima, Kunihiko},
  title = {{N}eocognitron: {A} Self-Organizing Neural Network Model 
    for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  journal = {Biological Cybernetics},
  pages = {193--202},
  volume = 36,
  year = 1980,
  doi = "10.1007/BF00344251"
}
@article{Fukushima:1988nc,
  author = {Kunihiko Fukushima},
  title = {Neocognitron: A hierarchical neural network capable of visual pattern recognition},
  journal = {Neural Networks},
  volume = {1},
  number = {2},
  pages = {119-130},
  year = {1988},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/0893-6080(88)90014-7},
  url = {https://www.sciencedirect.com/science/article/pii/0893608088900147}
}
@article{Fukushima:2019nc,
  title={Recent advances in the deep CNN neocognitron},
  author={Kunihiko Fukushima},
  journal={Nonlinear Theory and Its Applications, IEICE},
  volume={10},
  number={4},
  pages={304-321},
  year={2019},
  doi={10.1587/nolta.10.304}
}

% Early back-propagation network 
@incollection{Rumelhart:1986erp,
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  booktitle = {Parallel Distributed Processing: Explorations 
    in the Microstructure of Cognition, {V}olume 1: {F}oundations},
  editor = {Rumelhart, David E. and Mcclelland, James L.},
  pages = {318--362},
  publisher = {MIT Press},
  title = {Learning Internal Representations by Error Propagation},
  year = 1986
}

% Recurrent neural network for speech recognition (1991)
@article{Robinson:1991rerp,
  author = {Robinson, Tony and Fallside, Frank},
  journal = {Computer Speech and Language},
  pages = {259--274},
  title = {A recurrent error propagation network speech recognition system},
  volume = 5,
  number = 3,
  year = 1991,
  doi = {https://doi.org/10.1016/0885-2308(91)90010-N},
  url = {https://www.sciencedirect.com/science/article/pii/088523089190010N}
}

% Multilayer perceptron for speech recognition (1991) 
@inproceedings{Bengio:1991pma,
     author = {Bengio, Yoshua and De Mori, Renato and Flammia, Giovanni and Kompe, Ralf},
      title = {Phonetically motivated acoustic parameters for continuous speech recognition using artificial neural networks},
  booktitle = {Proceedings of EuroSpeech'91},
       year = {1991},
   location = {Genova, Italy},
cat={C},
}

% LeNet-5 (1998)
@inproceedings{LeCun:1998ln5,
  author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
  booktitle = {Proceedings of the IEEE},
  citeseerurl = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665},
  file = {:neural_nets/lecun-98.pdf:PDF;:lecun-98.pdf:PDF},
  groups = {public},
  number = 11,
  pages = {2278--2324},
  title = {Gradient-Based Learning Applied to Document Recognition},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665},
  volume = 86,
  year = 1998
}

% Mean field sigmoid belief network (1996)
@article{Saul:1996mf,
  author = {Saul, L. K., Jaakkola, T. and Jordan, M. I.},
  journal = {Journal of Artificial Intelligence},
  title = {Mean Field Theory for Sigmoid Belief Networks},
  volume = 4,
  year = 1996,
  eprint={9603102},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  doi = "https://doi.org/10.1613/jair.251"
}

% Echo state network (2004) (Jaeger and Haas, 2004)

% Deep belief network (2006) (Hinton et al., 2006)

% GPU-accelerated convolutional network (2006) (Chellapilla et al., 2006)

% Deep Boltzmann machine (2009) (Salakhutdinov and Hinton, 2009a)

% GPU-accelerated deep belief network (2009) (Raina et al., 2009)

% Unsupervised convolutional network (2009) (Jarrett et al., 2009)

% GPU-accelerated multilayer perceptron (2010) (Ciresan et al., 2010)

% OMP-1 network (2011) (Coates and Ng, 2011)

% Distributed autoencoder (2012) 
@inproceedings{Le:2012daut,
  author    = {Quoc V. Le and
               Marc'Aurelio Ranzato and
               Rajat Monga and
               Matthieu Devin and
               Greg Corrado and
               Kai Chen and
               Jeffrey Dean and
               Andrew Y. Ng},
  title     = {Building high-level features using large scale unsupervised learning},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning,
               {ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},
  year      = {2012},
  url       = {http://icml.cc/2012/papers/73.pdf}
}

% Multi-GPU convolutional network (2012)
@incollection{Krizhevsky:2012img,
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  booktitle = {Advances in Neural Information Processing Systems 25},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  pages = {1097--1105},
  publisher = {Curran Associates, Inc.},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  year = 2012
}

% COTS HPC unsupervised convolutional network (2013)
@inproceedings{Coates:2013cots,
  author = {Coates, Adam and Huval, Brody and Wang, Tao and Wu, David J. and Catanzaro, Bryan and Ng, Andrew Y.},
  booktitle = {ICML (3)},
  ee = {http://proceedings.mlr.press/v28/coates13.html},
  pages = {1337-1345},
  publisher = {JMLR.org},
  series = {JMLR Workshop and Conference Proceedings},
  title = {Deep learning with COTS HPC systems.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2013.html#CoatesHWWCN13},
  volume = 28,
  year = 2013
}

% GoogLeNet
@misc{Szegedy:2014gnet,
  title={Going Deeper with Convolutions}, 
  author={Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
  year={2014},
  eprint={1409.4842},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  doi = {https://doi.org/10.48550/arXiv.1409.4842}
}

% VGG
@misc{Simonyan:2015vgg,
  author={Karen Simonyan and Andrew Zisserman},
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
  year={2015},
  eprint={1409.1556},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

% AlexNet
@incollection{Krizhevsky:2012alexnet,
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  booktitle = {Advances in Neural Information Processing Systems 25},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  pages = {1097--1105},
  publisher = {Curran Associates, Inc.},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  year = 2012
}

% ZFNet
@inproceedings{Zaremba:2016zfnet,
  author = {Zaremba, Wojciech and Mikolov, Tomas and Joulin, Armand and Fergus, Rob},
  title = {Learning Simple Algorithms from Examples.},
  booktitle = {ICML},
  editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
  ee = {http://proceedings.mlr.press/v48/zaremba16.html},
  pages = {421-429},
  publisher = {JMLR.org},
  series = {JMLR Workshop and Conference Proceedings},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2016.html#ZarembaMJF16},
  volume = 48,
  year = 2016
}

% ResNet
@inproceedings{He:2016resnet,
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title = {{Deep Residual Learning for Image Recognition}},
  booktitle = {Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/CVPR.2016.90},
  issn = {1063-6919},
  pages = {770--778},
  publisher = {IEEE},
  series = {CVPR '16},
  url = {http://ieeexplore.ieee.org/document/7780459},
  year = 2016
}

% SVM
@article{Vapnik:1995svm,
    author = "Corina Cortes and Vladimir Vapnik",
    title = "{Support-vector networks}",
    doi = "10.1007/BF00994018",
    journal = "Machine Learning",
    volume = "20",
    number = "",
    pages = "273-297",
    year = "1995"
}

% No CNN pooling
@misc{Springenberg:2015pl,
  author={Jost Tobias Springenberg and Alexey Dosovitskiy and Thomas Brox and Martin Riedmiller},
  title={Striving for Simplicity: The All Convolutional Net}, 
  year={2015},
  eprint={1412.6806},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

% DCGAN (note: remove pooling)
@misc{Radford:2016dcgan,
  author={Alec Radford and Luke Metz and Soumith Chintala},
  title={Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}, 
  year={2016},
  eprint={1511.06434},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
%
% Other articles
%

% reference for schematic showing the increased receptive field in deeper CNN layers
@article{Lin:2017rf,
author = {Lin, Haoning and Shi, Zhenwei and Zou, Zhengxia},
year = {2017},
month = {05},
pages = {480},
title = {Maritime Semantic Labeling of Optical Remote Sensing Images with Multi-Scale Fully Convolutional Network},
volume = {9},
journal = {Remote Sensing},
doi = {10.3390/rs9050480}
}

% reference for schematic showing the sigmoid activation and its derivative
@article{Xiang:2022ato,
  author = {Xiang, Cheng and Dalei, Wang and Pan, Yue and Chen, Airong and Zhou, Xiaoyi and Zhang, Yiquan},
  year = {2022},
  month = {03},
  pages = {},
  title = {Accelerated topology optimization design of 3D structures based on deep learning},
  volume = {65},
  journal = {Structural and Multidisciplinary Optimization},
  doi = {10.1007/s00158-022-03194-0}
}

% reference for schematic showing the common activation functions and their derivatives
@misc{Ostwald:2021bpi,
  title={An induction proof of the backpropagation algorithm in matrix notation}, 
  author={Dirk Ostwald and Franziska Usée},
  year={2021},
  eprint={2107.09384},
  archivePrefix={arXiv},
  primaryClass={stat.ML}
}

%
% Web pages
%

@online{OED,
    title = "{Oxford English Dictionary}",
    url = {https://www.oed.com/}
}

% reference for plot showing example f(x), f'(x), and f''(x) and
% illustrating the decond derivative test
@online{TechUK:2ndDerivativeTest,
  author = {},
  title = {The Second Derivative Test},
  year = {},
  url = {https://www.technologyuk.net/mathematics/differential-calculus/second-derivative-test.shtml},
  urldate = {Accessed: 2023-10-03}
}

% no free luch theorems
@online{NoFreeLunch,
  author = {},
  title = {No Free Lunch Theorems},
  year = {},
  url = {http://www.no-free-lunch.org/},
  urldate = {Accessed: 2023-09-13}
}

% reference for schematic showing saddle point in 2D
@online{Wikipedia:SaddlePoint,
  author = {},
  title = {Saddle point},
  year = {},
  url = {https://en.wikipedia.org/wiki/Saddle_point},
  urldate = {Accessed: 2023-10-03}
}

@online{Wikipedia:OccamRazor,
  author = {},
  title = {Occam's razor},
  year = {},
  url = {https://en.wikipedia.org/wiki/Occam%27s_razor},
  urldate = {Accessed: 2023-09-13}
}

@online{Wikipedia:HistoryOfAI,
  author = {},
  title = {History of artificial intelligence},
  year = {},
  url = {https://en.wikipedia.org/wiki/History_of_artificial_intelligence},
  urldate = {Accessed: 2023-02-20}
}

@online{Wikipedia:VisualCortex,
  author = {},
  title = {Visual cortex},
  year = {},
  url = {https://en.wikipedia.org/wiki/Visual_cortex},
  urldate = {Accessed: 2023-04-02}
}

@online{Wikipedia:Convolution,
  author = {},
  title = {Convolution},
  year = {},
  url = {https://en.wikipedia.org/wiki/Convolution},
  urldate = {Accessed: 2023-04-02}
}

@online{Wikipedia:LinearSeparability,
  author = {},
  title = {Linear separability},
  year = {},
  url = {https://en.wikipedia.org/wiki/Linear_separability},
  urldate = {Accessed: 2023-03-08}
}

@online{Wikipedia:SVM,
  author = {},
  title = {Support vector machine},
  year = {},
  url = {https://en.wikipedia.org/wiki/Support_vector_machine},
  urldate = {Accessed: 2023-03-08}
}

@online{Scholarpedia:AreaV1,
  author = {},
  title = {Area V1},
  year = {},
  url = {http://www.scholarpedia.org/article/Area_V1},
  urldate = {Accessed: 2023-04-02}
}

@online{NVidiaBlog:DifferenceBetweenAIMLDL,
  author = {},
  title = {What’s the Difference Between Artificial Intelligence, Machine Learning and Deep Learning?},
  year = {},
  url = {https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/},  
  urldate = {Accessed: 2023-02-22}
}

@online{KerasBlog:BuildingAutoencodersInKeras,
  author = {},
  title = {Building Autoencoders in Keras},
  year = {},
  url = {https://blog.keras.io/building-autoencoders-in-keras.html},
  urldate = {Accessed: 2023-03-21}
}

@online{AIPlainEng:RiseAndFallOfPerceptron,
  author = {},
  title = {The Rise and Fall of the Perceptron},
  year = {},
  url = {https://ai.plainenglish.io/the-rise-and-fall-of-the-perceptron-c04ae53ea465},  
  urldate = {2023-02-23}
}

@online{HarvardBrainTour:HubelAndWiesel,
  author = {},
  title = {A Nobel Partnership: Hubel and Wiesel},
  year = {},
  url = {https://braintour.harvard.edu/archives/portfolio-items/hubel-and-wiesel},  
  urldate = {2023-04-04}
}

@online{GoodPsychology:HubelAndWiesel,
  author = {},
  title = {Feature Detectors: Hubel and Wiesel experiment},
  year = {},
  url = {https://goodpsychology.wordpress.com/2013/03/13/235/},  
  urldate = {2023-04-04}
}

% reference for schematic showing loc min/max and saddle points in 2D
@online{OffConvex:EscapingSaddlePoints,
  author = {Rong Ge},
  title = {Escaping from Saddle Points},
  year = {2016},
  url = {https://www.offconvex.org/2016/03/22/saddlepoints/},  
  urldate = {2023-09-29}
}

% reference for schematic illustrating gradient descent in 2D
@online{Medium:GradDescentOptLinReg,
  author = {Joshgun Guliyev},
  title = {Explanation of Gradient Descent Optimization Algorithm on Linear Regression example},
  year = {2020},
  url = {https://medium.com/analytics-vidhya/gradient-descent-b0dc1af33517},  
  urldate = {2023-09-29}
}

% reference for schematic illustrating the momentum method, usin a ball rolling down a bowl
@online{Medium:GradDescentMomRMSPropAdam,
  author = {Harsh Khandewal},
  title = {Gradient Descent with Momentum, RMSprop And Adam Optimizer},
  year = {2020},
  url = {https://medium.com/analytics-vidhya/momentum-rmsprop-and-adam-optimizer-5769721b4b19},  
  urldate = {2023-10-26}
}

% reference for schematic illustrating the momentum method, usin a ball rolling down a bowl
@online{TowardsDataScience:LP2MomentumBasedGS,
  author = {Akshay L Chandra},
  title = {Learning Parameters, Part 2: Momentum-Based and Nesterov Accelerated Gradient Descent},
  year = {2019},
  url = {https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12},  
  urldate = {2023-10-26}
}

% reference for schematic showing the workings of neocognitron
% reference for schematic showing the LeNet-5 architecture
@online{Medium:LeNet5andAlexNet,
  author = {},
  title = {The Convolutional Neural Network — Theory and Implementation of LeNet-5 and AlexNet},
  year = {},
  url = {https://medium.com/@zahraelhamraoui1997/the-convolutional-neural-network-theory-and-implementation-of-lenet-5-and-alexnet-5266e4577e96},  
  urldate = {2023-04-05}
}

% reference for schematic showing low/mid/high level features for an image
@online{Medium:FeatureMaps,
  author = {Chris Kevin},
  title = {Feature Maps},
  year = {2018},
  url = {https://medium.com/@chriskevin_80184/feature-maps-ee8e11a71f9e},  
  urldate = {2023-04-05}
}

% reference for schematic showing the impact of low/right/high learning rate
@online{Medium:GradDescent,
  author = {Atulanand},
  title = {Gradient Descent},
  year = {2022},
  url = {https://medium.com/codex/gradient-descent-cb0f02dc6eab},  
  urldate = {2023-10-13}
}

% reference for schematic showing loss function for various learning rates
@online{Medium:LRateDecay,
  author = {Parth Lathiya},
  title = {Loss explosion while training a custom model},
  year = {2020},
  url = {https://parthlathiya.medium.com/loss-explosion-while-training-a-custom-model-ad15fbc00d1},  
  urldate = {2023-10-24}
}

% reference for cat image showing input to CNN and explaining RGB colour channels
@online{DevCommunity:GoingFurtherWithCNN,
  author = {},
  title = {Machine Learning - Going Further with CNN},
  year = {},
  url = {https://dev.to/sandeepbalachandran/machine-learning-going-furthur-with-cnn-part-2-41km},  
  urldate = {2023-04-04}
}

% reference for schematic showing the classic CNN architecture 
@online{TowardsDataScience:BasicsOfClassicCNN,
  author = {Chandra Churh Chatterjee},
  title = {Basics of the classic CNN},
  year = {2019},
  url = {https://towardsdatascience.com/basics-of-the-classic-cnn-a3dce1225add},  
  urldate = {2023-04-04}
}

% reference for CNN illustrations
@online{TowardsDataScience:AppliedDL4,
  author = {Arden Dertat},
  title = {Applied Deep Learning - Part 4: Convolutional Neural Networks},
  year = {2017},
  url = {https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2},  
  urldate = {2023-04-06}
}

% reference for schematic showing the shrinking spatial extend and 
% increasing depth of CNN layers
@online{HuiBlog:CNNTutorial,
  author = {Jonathan Hui},
  title = {Convolutional neural networks (CNN) tutorial},
  year = {2017},
  url = {https://jhui.github.io/2017/03/16/CNN-Convolutional-neural-network/},  
  urldate = {2023-04-12}
}

% reference for CNN pooling illustrations
@online{PyImageSearch:CNNLayerTypes,
  author = {Adrian Rosebrock},
  title = {Convolutional Neural Networks (CNN) and Layer Types},
  year = {2021},
  url = {https://pyimagesearch.com/2021/05/14/convolutional-neural-networks-cnns-and-layer-types/},  
  urldate = {2023-04-12}
}

% reference for photo showing the only surviving SNARC neuron
@online{CyberneticZoo:1951MazeSolver,
  author = {},
  title = {1951 - SNARC maze solver},
  year = {2009},
  url = {https://cyberneticzoo.com/mazesolvers/1951-maze-solver-minsky-edmonds-american/},  
  urldate = {2023-04-12}
}

% Javascript CNN demo, working with CIFAR-10
@online{Demo:ConvNetJSCIFAR10,
  author = {Andrej Karpathy},
  title = {ConvNetJS CIFAR-10 demo},
  year = {2009},
  url = {https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html},  
  urldate = {2023-04-12}
}

% Schematic illustrating underfitting and overfitting
@online{MathWorks:Overfitting,
  author = {},
  title = {Mathworks: Overfitting},
  year = {},
  url = {https://www.mathworks.com/discovery/overfitting.html},  
  urldate = {2023-09-12}
}

% Schematic illustrating a 2D function with multiple local minima
@online{StackExch:MultipleMinima,
  author = {},
  title = {Stack Exchange: Function with multiple local minima},
  year = {},
  url = {https://stats.stackexchange.com/questions/279363/function-with-multiple-local-minima},  
  urldate = {2023-09-12}
}

