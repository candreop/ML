
\begin{frame}[t]{The `no free lunch' theorem}

Can we really infer general rules from a finite set of training examples?
\begin{itemize}
  \item Inductive reasoning is {\bf intrinsically uncertain}.
  \item \gls{ml} does not offer any rule with certainty - 
  only rules that are {\bf {\em probably} correct for {\em most} input data examples}.
\end{itemize}

\vspace{0.1cm}

\begin{blockexample}{}
\underline{The `{\bf no free lunch theorem}' of  \gls{ml}} 
by D.Wolpert and W.Macready \cite{NoFreeLunch}.\\
\vspace{0.1cm}
{\small
Averaged over all data-generating distributions,
every classification algorithm has the same Generalisation performance
against previously unseen examples.\\
}
\end{blockexample}

\vspace{0.1cm}

The `no free lunch theorem' implies there is 
{\bf no universal learning algorithm} 
performing well for all possible data-generating distributions.

\vspace{0.2cm}

However, by building in some assumptions,
{\bf we can design algorithms that work well in specific real-world application}.

\end{frame}
