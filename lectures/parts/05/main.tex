\renewcommand{\thispart}{5 }
\renewcommand{\thispartname}{Practical Issues in Neural Network Training}

\part{\thispartname}

% Cover page
\section{Outline}
\input{common/cover_part.tex}

% Outline
\input{common/toc_part.tex}

\section{Basics of gradient-based optimization}
\input{parts/05/gradient_based_optimization}
\section{Beyond the gradient: The Jacobian matrix}
\input{parts/05/beyond_the_gradient_jacobian}
\section{Beyond the gradient: The Hessian matrix}
\input{parts/05/beyond_the_gradient_hessian}

\section{Vanishing and exploding gradient problems}
%  - Leaky ReLU and maxout 

\section{Difficulties in convergence}
\section{Local optima}

\section{Generalisation}
\input{parts/05/generalisation.tex}

\section{Capacity, overfitting and underfitting}
\input{parts/05/over_and_underfitting.tex}
\input{parts/05/capacity.tex}

\section{No free lunch theorem}
\input{parts/05/no_free_lunch_theorem.tex}

\section{Bias-variance trade-off}
\input{parts/05/estimators.tex}
\input{parts/05/bias.tex}
\input{parts/05/variance.tex}
\input{parts/05/bias_variance_examples.tex}
\input{parts/05/bias_variance_tradeoff.tex}
\input{parts/05/mle.tex}

\section{Regularisation methods for deep learning}
\input{parts/05/regularisation.tex}

\subsection{Norm penalties: L2 and L1 regularisation}
\subsection{Dataset augmentation}
\subsection{Noise robustness}
\subsection{Multitask learning}
\subsection{Early stopping}
\subsection{Parameter sharing}
\subsection{Ensemble methods: Bagging, subsampling and dropout}
\subsection{Adversarial Training}


% Suggested reading for this part
\section{Suggested reading}
\input{parts/02/reading.tex}
