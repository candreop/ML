\renewcommand{\thispart}{5 }
\renewcommand{\thispartname}{Practical Issues in Neural Network Training}

\part{\thispartname}

% Cover page
\section{Outline}
\input{common/cover_part.tex}

% Outline
\input{common/toc_part.tex}

\section{Basics of gradient-based optimization}
\input{parts/05/gradient_based_optimization}
\section{Beyond the gradient}
\subsection{The Jacobian matrix}
\input{parts/05/beyond_the_gradient_jacobian}
\subsection{The Hessian matrix}
\input{parts/05/beyond_the_gradient_hessian}

\section{Difficulties in convergence}

\subsection{Vanishing and exploding gradient problems}
%  - Leaky ReLU and maxout 
\subsection{Local optima}

\section{Generalisation}
\input{parts/05/generalisation.tex}

\section{Capacity, overfitting and underfitting}
\input{parts/05/over_and_underfitting.tex}
\input{parts/05/capacity.tex}

\section{No free lunch theorem}
\input{parts/05/no_free_lunch_theorem.tex}

\section{Introduction to regularisation}
\input{parts/05/regularisation.tex}

\section{Bias-variance trade-off}
\input{parts/05/estimators.tex}
\input{parts/05/bias.tex}
\input{parts/05/variance.tex}
\input{parts/05/bias_variance_examples.tex}
\input{parts/05/bias_variance_tradeoff.tex}
\input{parts/05/mle.tex}

\section{Regularisation revisited}
\subsection{Norm penalties: L2 and L1 regularisation}
\subsection{Dataset augmentation}
\subsection{Noise robustness}
\subsection{Multitask learning}
\subsection{Early stopping}
\subsection{Parameter sharing}
\subsection{Ensemble methods: Bagging, subsampling and dropout}
\subsection{Adversarial Training}


% Suggested reading for this part
\section{Suggested reading}
\input{parts/02/reading.tex}
