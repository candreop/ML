\renewcommand{\prevpart}{5 }
\renewcommand{\thispart}{6 }
\renewcommand{\nextpart}{7 }

\section{Convolutional Neural Networks}

% Cover page
\input{common/cover_part.tex}

% Outline
\input{common/toc_part.tex}

%
%
%

\begin{frame}[t,allowframebreaks]{Convolutional Neural Networks}

    A \index{CNN}\index{convolutional neural network}\gls{cnn} 
    is a specialised type of deep neural network designed for use 
    with {\bf grid-structured inputs}.\\
    Examples include:
    \begin{itemize}
      \item {\bf time series} (1-D grid-structured data), or 
      \item {\bf images} (2-D grid-structured data).
    \end{itemize}

    The vast majority of \gls{cnn} applications focusses on image data.
    In this type of data exhibits:
    \begin{itemize}
        \item Some degree of {\bf similarity in adjacent grid locations}.\\
          {\it For example, neighbouring pixels often have a similar colour.}
        \item A degree of {\bf translation invariance}.\\
          {\it For example, our interpretation of an object as a "cat"
          does not depend on where it appears on the image.}
    \end{itemize}

    \framebreak

    \glspl{cnn} use \index{convolution} \gls{convolution}
    operations in at least one of their layers.
    Convolution:
    \begin{itemize}
     \item is used in place of general matrix multiplication,
     \item enables parameter sharing 
    \end{itemize}

\end{frame}


\subsection{Biological inspirations}
\input{parts/06/biological_inspirations.tex}

%
%
%

\begin{frame}[t,allowframebreaks]{Neocognitron -}

    In 1908, K. Fukushima \cite{Fukushima:1980nc, Fukushima:1988nc}, 
    inspired by the biological visual cortex work of T.Hubel and T.Wiesel,
    created the \index{neocognitron}\gls{neocognitron}
    as a model of the visual system.

    The \gls{neocognitron} is the precursor of the \gls{cnn}.

    \framebreak

    \begin{center}
        \includegraphics[width=0.90\textwidth]
           {./images/neocognitron/fukushima88_hierarchical_network_structure_01.png}\\
        Hierarchical network structure of the \gls{neocognitron}.\\
        {\scriptsize \color{col:attribution} 
        Image reproduced from \cite{Fukushima:1988nc} (Fig.2)}\\
    \end{center}

\end{frame}

%
%
%

\subsection{The convolution and cross-correlation operations}
\input{parts/06/convolution.tex}

\subsection{Motivation}
\begin{frame}[t,allowframebreaks]{Ad}

    \glspl{cnn} use \index{convolution} \gls{convolution}
    operations in at least one of their layers, instead of general matrix multiplication.

    As it is discussed in \cite{Goodfellow:2017MITDL} 
    (see Sec. 9.2 for a detailed discussion), \gls{convolution} 
    enables:
    \begin{itemize}
        \item Sparse connectivity
        \item Parameter sharing
        \item Equivariant representations
    \end{itemize}

\end{frame}

\subsubsection{Sparse connectivity}
\begin{frame}[t,allowframebreaks]{Sparse connectivity -}

    \begin{columns}
        \begin{column}{0.50\textwidth}
         \begin{center}
          \includegraphics[width=1.0\textwidth]
          {./images/cnn/sparse_connectivity/goodfellow17_sparse_connectivity_from_above_01.png}\\
          {\scriptsize \color{col:attribution} 
          Image reproduced from p.327 of \cite{Goodfellow:2017MITDL}}\\
         \end{center}
        \end{column}
        \begin{column}{0.50\textwidth}
        \end{column}
    \end{columns}

    \framebreak

    \begin{columns}
        \begin{column}{0.50\textwidth}
         \begin{center}
          \includegraphics[width=1.0\textwidth]
          {./images/cnn/sparse_connectivity/goodfellow17_sparse_connectivity_from_below_01.png}\\
          {\scriptsize \color{col:attribution} 
          Image reproduced from p.327 of \cite{Goodfellow:2017MITDL}}\\
         \end{center}
        \end{column}
        \begin{column}{0.50\textwidth}
        \end{column}
    \end{columns}

\end{frame}

%
%
%

\begin{frame}[t,allowframebreaks]{Parameter sharing -}

\end{frame}

%
%
%

\begin{frame}[t,allowframebreaks]{Equivariant representations -}

\end{frame}

%
%
%

\begin{frame}[t]{Basic structure of a Convolutional Neural Network}

    Typically, a \index{CNN}\index{convolutional neural network}\gls{cnn} layer
    consists of {\bf 3 stages}:
    \begin{enumerate}
        \item {\bf Convolution} stage:
           Performing convolutions in parallel. 
           Linear activations           
        \item {\bf Detector} stage:
        \item {\bf Pooling} stage:
    \end{enumerate}

\end{frame}
