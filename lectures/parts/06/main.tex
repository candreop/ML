\renewcommand{\prevpart}{5 }
\renewcommand{\thispart}{6 }
\renewcommand{\nextpart}{7 }

\section{Convolutional Neural Networks}

% Cover page
\input{common/cover_part.tex}

% Outline
\input{common/toc_part.tex}

%
%
%

\begin{frame}[t,allowframebreaks]{Convolutional Neural Networks}

    A \index{CNN}\index{convolutional neural network}\gls{cnn} 
    is a specialised type of deep neural network designed for use 
    with {\bf grid-structured inputs}.\\
    Examples include:
    \begin{itemize}
      \item {\bf time series} (1-D grid-structured data), or 
      \item {\bf images} (2-D grid-structured data).
    \end{itemize}

    The vast majority of \gls{cnn} applications focusses on image data.
    In this type of data exhibits:
    \begin{itemize}
        \item Some degree of {\bf similarity in adjacent grid locations}.\\
          {\it For example, neighbouring pixels often have a similar colour.}
        \item A degree of {\bf translation invariance}.\\
          {\it For example, our interpretation of an object as a "cat"
          does not depend on where it appears on the image.}
    \end{itemize}

    \framebreak

    \glspl{cnn} use \index{convolution} \gls{convolution}
    operations in at least one of their layers.
    Convolution:
    \begin{itemize}
     \item is used in place of general matrix multiplication,
     \item enables parameter sharing 
    \end{itemize}

\end{frame}

% Discuss studies of the primary visual cortex (V1) by Hubel, Wiesel and others,
% and describe the workings of V1
\subsection{Biological inspirations}
\input{parts/06/biological_inspirations.tex}

% Discuss the Neocognitron
\subsection{Convolutional Network precursors: Neocognitron}
\input{parts/06/neocognitron.tex}

% Give a list of modern architectures, but defer detailed description 
% till after the basics of convolutional networks are described
\subsection{From the Neocognitron to modern convolutional architectures}
\input{parts/06/from_neocognitron_to_modern_architectures.tex}


%
%
%

\subsection{The convolution and cross-correlation operations}
\input{parts/06/convolution.tex}

\subsection{Motivation}
\begin{frame}[t,allowframebreaks]{Ad}

    \glspl{cnn} use \index{convolution} \gls{convolution}
    operations in at least one of their layers, instead of general matrix multiplication.

    As it is discussed in \cite{Goodfellow:2017MITDL} 
    (see Sec. 9.2 for a detailed discussion), \gls{convolution} 
    enables:
    \begin{itemize}
        \item Sparse connectivity
        \item Parameter sharing
        \item Equivariant representations
    \end{itemize}

\end{frame}

\subsubsection{Sparse connectivity}
\begin{frame}[t,allowframebreaks]{Sparse connectivity -}

    \begin{itemize}
        \item 
        In the fully connected neural networks described earlier in this module, 
        {\bf every output unit interacts with every input unit} 
    \end{itemize}

    \framebreak

    \begin{columns}
        \begin{column}{0.50\textwidth}
         \begin{center}
          \includegraphics[width=1.0\textwidth]
          {./images/cnn/sparse_connectivity/goodfellow17_sparse_connectivity_from_above_01.png}\\
          {\scriptsize \color{col:attribution} 
          Image reproduced from p.327 of \cite{Goodfellow:2017MITDL}}\\
         \end{center}
        \end{column}
        \begin{column}{0.50\textwidth}
        \end{column}
    \end{columns}

    \framebreak

    \begin{columns}
        \begin{column}{0.50\textwidth}
         \begin{center}
          \includegraphics[width=1.0\textwidth]
          {./images/cnn/sparse_connectivity/goodfellow17_sparse_connectivity_from_below_01.png}\\
          {\scriptsize \color{col:attribution} 
          Image reproduced from p.327 of \cite{Goodfellow:2017MITDL}}\\
         \end{center}
        \end{column}
        \begin{column}{0.50\textwidth}
        \end{column}
    \end{columns}

\end{frame}

%
%
%

\begin{frame}[t,allowframebreaks]{Parameter sharing -}

\end{frame}

%
%
%

\begin{frame}[t,allowframebreaks]{Equivariant representations -}

\end{frame}

%
%
%

\begin{frame}[t]{Basic structure of a Convolutional Neural Network}

    Typically, a \index{CNN}\index{convolutional neural network}\gls{cnn} layer
    consists of {\bf 3 stages}:
    \begin{enumerate}
        \item {\bf Convolution} stage:
           Performing convolutions in parallel. 
           Linear activations           
        \item {\bf Detector} stage:
        \item {\bf Pooling} stage:
    \end{enumerate}

\end{frame}

\subsection{Case studies}


\subsubsection{LeNet-5}

\subsubsection{AlexNet}

\subsubsection{GoogLeNet}

\subsubsection{ResNet}
