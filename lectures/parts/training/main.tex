\renewcommand{\thispart}{4 }
\renewcommand{\thispartname}{Learning Algorithms and Practical Issues in Neural Network Training}

\part{\thispartname}

% Cover page
\section{Outline}
\input{common/cover_part.tex}

% Outline
\input{common/toc_part.tex}

% Learning objectives
\input{parts/training/learning_objectives}

\section{Basics of gradient-based optimization}
\input{parts/training/gradient_based_optimization}
\section{Beyond the gradient}
\subsection{The Jacobian matrix}
\input{parts/training/beyond_the_gradient_jacobian}
\subsection{The Hessian matrix}
\input{parts/training/beyond_the_gradient_hessian}

\section{Optimisation in ML}
\input{parts/training/optimisation_in_ml}

\section{Gradient-based optimisation in ML}
\input{parts/training/gradient_based_optimization_in_ml}

\section{Challenges in ML model optimisation}
\input{parts/training/challenges_in_optimisation}
\subsection{Vanishing and exploding gradient problems}
\input{parts/training/vanishing_and_exploding_gradients}
%  - Leaky ReLU and maxout 
\subsection{Local minima}
\input{parts/training/local_minima}

\section{Gradient descent strategies}

\subsection{Learning rate decay}
\input{parts/training/gradient_descent_strategy_rate_decay}

\subsection{Momentum-based strategies}
\input{parts/training/gradient_descent_strategy_momentum}
\subsubsection{Nesterov momentum}
\input{parts/training/gradient_descent_strategy_momentum_nesterov}

\subsection{Adaptive sub-gradient methods}
\input{parts/training/gradient_descent_strategy_adaptive_rates}
\subsubsection{Delta-bar-delta algorithm}
\input{parts/training/gradient_descent_strategy_adaptive_rates_deltabardelta}
\subsubsection{AdaGrad algorithm}
\input{parts/training/gradient_descent_strategy_adaptive_rates_adagrad}
\subsubsection{RMSprop algorithm}
\input{parts/training/gradient_descent_strategy_adaptive_rates_rmsprop}
\subsubsection{Adam algorithm}
\input{parts/training/gradient_descent_strategy_adaptive_rates_adam}
\subsection{Second-order methods}
\input{parts/training/gradient_descent_strategy_2nd_derivatives}
\subsubsection{Newton's method}
\input{parts/training/gradient_descent_strategy_2nd_derivatives_newton}
\subsubsection{Approximate methods}
\input{parts/training/gradient_descent_strategy_2nd_derivatives_approximate}

\subsection{Gradient clipping methods}


\section{Generalisation}
\input{parts/training/generalisation.tex}

\section{Capacity, overfitting and underfitting}
\input{parts/training/over_and_underfitting.tex}
\input{parts/training/capacity.tex}

\section{No free lunch theorem}
\input{parts/training/no_free_lunch_theorem.tex}

\section{Introduction to regularisation}
\input{parts/training/regularisation.tex}

\section{Bias-variance trade-off}
\input{parts/training/estimators.tex}
\input{parts/training/bias.tex}
\input{parts/training/variance.tex}
\input{parts/training/bias_variance_examples.tex}
\input{parts/training/bias_variance_tradeoff.tex}
\input{parts/training/mle.tex}

\section{Regularisation revisited}
\subsection{Norm penalties: L2 and L1 regularisation}
\subsection{Dataset augmentation}
\subsection{Noise robustness}
\subsection{Multitask learning}
\subsection{Early stopping}
\subsection{Parameter sharing}
\subsection{Ensemble methods: Bagging, subsampling and dropout}
\subsection{Adversarial Training}

% Suggested reading for this part
\section{Suggested reading}
\input{parts/training/reading.tex}
