%
% Intro to Automatic Differentiation
%

\begin{frame}[t,allowframebreaks]{Introduction to Automatic Differentiation -} 

  \index{AD}\index{automatic differentiation}\gls{ad} 
  is a technique to {\bf evaluate the derivative
  of a function} specified by a 
  \gls{computation graph}.\\
  \vspace{0.2cm}

  It is also called
  \index{algorithmic differentiation}\gls{algorithmic differentiation}, or
  \index{computational differentiation}\gls{computational differentiation}.\\
  \vspace{0.2cm}

  The technique works by exploiting the function decomposition expressed in the
  computation graph, and {\bf applying the chain rule repeatedly}.\\
  \vspace{0.2cm}
  
  \gls{ad} has a number of advantages:
  \begin{itemize}
    \item It is easily programmable.
    \item It is {\bf efficient}.
    \item It is {\bf numerically stable}.
  \end{itemize}
  
  \framebreak
  
  Note that automatic differentiation {\bf differs from both}:
  \begin{itemize}
    \item {\bf symbolic differentiation}, and
    \item {\bf numerical differentiation}
  \end{itemize}
  
  \begin{blockexample}{Symbolic differentiation}
  \end{blockexample}

  \begin{blockexample}{Numerical differentiation}
  \end{blockexample}

\end{frame}

%
% Automatic Differentiation: How it works
%

\begin{frame}[t,allowframebreaks]{Automatic Differentiation: How it works -} 

  For a given function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$,
  the corresponding \index{Jacobian matrix}\gls{Jacobian matrix} 
  $\vect{J}_f$ has $m$ rows and $n$ columns.
  Its element at row $i$ and column $j$, is given by:
  
  \begin{equation}
    {J_f}_{ij} = \frac{\partial f_i}{\partial w_j}
    \label{eq:ad_jacobian_element_1}
  \end{equation}
  
  where $f_i$ is the $i^{th}$ element of the function $f=(f_1, f_2, ..., f_m)$,
  and $w_j$ is the $j^{th}$ element of the input vector $\vect{w}=(w_1, w_2, ..., w_n)$.\\
  
  \vspace{0.2cm}
  
  Suppose that the function $f$ is a composite function:
  \begin{equation}
    \vect{f}(\vect{w}) = 
      (\vect{h} \circ \vect{g}) (\vect{w})= \vect{h}(\vect{g}(\vect{w}))
    \label{eq:ad_composite_function_1}
  \end{equation}
  
  with $\vect{w} \in \mathbb{R}^n$, 
  $g: \mathbb{R}^n \rightarrow \mathbb{R}^k$, and
  $h: \mathbb{R}^k \rightarrow \mathbb{R}^m$.
  
  \framebreak
  
  Applying the chain rule, 
  the element at row $i$ and column $j$
  of the $m \times n$ \index{Jacobian matrix}\gls{Jacobian matrix} $\vect{J}$ of 
  the composite function $\vect{f}$ is:
  \begin{equation}
    {J_f}_{ij} = 
      \frac{\partial f_i}{\partial w_j} = 
      \sum_{k} \frac{\partial f_i}{\partial g_k} \frac{\partial g_k}{\partial w_j}
      \label{eq:ad_jacobian_element_composite_function_1}
  \end{equation}
  
  In more compact notation, 
  the \gls{Jacobian matrix} $\vect{J}_{f}$ can be written as:
  \begin{equation}
    J_{f} = J_{h} J_{g}
      \label{eq:ad_jacobian_composite_function_1}
  \end{equation}
  
  More generally, if $\vect{f}$ is a composite of $\ell$ functions $f_1$,...,$f_\ell$:
  \begin{equation}
    \vect{f}(\vect{w}) = 
      (\vect{f_{\ell}} \circ \vect{f_{\ell-1}} \circ ... 
        \circ \vect{f_1})(\vect{w})= \vect{f_{\ell}}(\vect{f_{\ell-1}}(...(\vect{f_1}(\vect{w}))))
    \label{eq:ad_composite_function_2}  
  \end{equation}
  
  the corresponding 
  \index{Jacobian matrix}\gls{Jacobian matrix} $\vect{J}_{f}$ 
  can be written as:
  \begin{equation}
    J_{f} = J_{f_{\ell}} J_{f_{\ell-1}} ... J_{f_1}
    \label{eq:ad_jacobian_composite_function_2}
  \end{equation}
  
  \framebreak
  
  There are two distinct types of automatic differentiation:
  
  \begin{itemize}
    \item 
      Forward mode of automatic differentiation 
      (or forward accumulation).\\
      In this mode, we traverse the chain rule from inside to outside:\\
      First we evaluate $\displaystyle \frac{\partial f_1}{\partial w}$,
      then $\displaystyle \frac{\partial f_2}{\partial f_1}$,
      and at last $\displaystyle \frac{\partial f_\ell}{\partial f_{\ell-1}}$
    \item 
      Reverse mode of automatic differentiation 
      (or reverse accumulation).\\
      In this mode, we traverse the chain rule from outside to inside.
  \end{itemize}
  
  \vspace{0.2cm}
  
  Given a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$,
  \begin{itemize}
    \item 
      the forward mode is more efficient if $n << m$, and
    \item 
      the reverse mode is more efficient if $n >> m$.
  \end{itemize}
  
\end{frame}
  
  