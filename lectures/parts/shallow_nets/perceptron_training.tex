%
%
%

\begin{frame}[t]{How does the perceptron learn?}

    Our goal is to {\bf reduce the number of misclassifications}.\\
    \vspace{0.3cm}

    The error of the prediction is:
    \begin{equation}
        E(\vect{x}) = y - \hat{y}
        \label{eq:perceptron_prediction_error}
    \end{equation}        
    where $\hat{y}$ is given by Eq. \ref{eq:perceptron_prediction_wo_bias} 
    (or Eq. \ref{eq:perceptron_prediction_w_bias})\\
    \vspace{0.5cm}

    $E(\vect{x})$ takes values from the set: \{ -2, 0, +2 \}.\\
    \begin{center}
        \begin{tabular}{rrcr}
            \hline
            $y$  & $\hat{y}$ & classification & $E(\vect{x})$ \\
            \hline
            -1   & -1  &   correct &   0  \\
            -1   & +1  &   wrong   &  -2  \\
            +1   & -1  &   wrong   &  +2  \\
            +1   & +1  &   correct &   0  \\
            \hline
    \end{tabular}
    \end{center}

    If $E(\vect{x}) \ne 0$, the {\bf weights need to be updated}, but how?\\

\end{frame}

%
%
%

\begin{frame}[t]{How does the perceptron learn?}

    The learning process follows the {\em error gradient}.\\
    \vspace{0.4cm}   

    Almost all neural network learning problems are expressed in a similar way, 
    with the minimization of a suitable 
    \index{loss function}\gls{loss function} (or 
    \index{objective function}\gls{objective function}) $L$.\\
    \begin{equation}
        \vect{w} \rightarrow \vect{w^\prime} = \vect{w} - \alpha \nabla_{\vect{w}} L
        \label{eq:weight_update_grad_descent}
    \end{equation}
    where $\alpha$ regulates the learning rate.\\
    \vspace{0.4cm}

    {\color{red}grad descent}
    In the \index{perceptron}\gls{perceptron},
    as with all \index{linear model}\glspl{linear model},
    the process tries to determine the linear hyperplane: 
    \begin{equation}
        \vect{w}^T \cdot \vect{x} = 0
        \label{eq:percepton_hyperplane_1}
    \end{equation} 
    separating the training instances with $y$=+1 
    from those with $y$=-1.\\
    \vspace{0.2cm}
    \noindent\rule{4cm}{0.4pt}\\
    {\small
        Note: $\vect{w}^T \cdot \vect{x}$ 
        is the argument of the {\em sign} function computed by the 
        \index{perceptron}\gls{perceptron}.\\
    }

\end{frame}


%
%
%

\begin{frame}[t]{How does the perceptron learn?}
    
    \begin{center}
        \includegraphics[width=0.55\textwidth]{./images/perceptron/data_linearly_separable.png}\\
        {\scriptsize 
        The hyperplane $H_3$ provides better separation than $H_1$ and $H_2$.
        \color{col:attribution} 
        Image reproduced from \cite{Wikipedia:LinearSeparability}}\\
    \end{center}
    \vspace{0.2cm}
    What if the data is {\bf \underline{not} linearly separable}?\\
    The \index{perceptron}\gls{perceptron} performs poorly in that case, but we will discuss this later.\\

\end{frame}



%
%
%

\begin{frame}[t]{Heuristic update process of the Mark 1 perceptron}

    As we have seen (Eq. \ref{eq:weight_update_grad_descent}), 
    the \index{perceptron}\gls{perceptron} 
    learning algorithm can be expressed as:
    \begin{equation*}
        \vect{w} \rightarrow \vect{w^\prime} = \vect{w} - \alpha \nabla_{\vect{w}} L
    \end{equation*}

    Mark 1 used a {\bf heuristic weight optimization}, performed in hardware.\\
    It was not derived from a \index{loss function}\gls{loss function} 
    using \index{gradient}\index{gradient descent}\gls{gradient descent}.\\
    \vspace{0.2cm} 
    Updates were designed to minimise the number of misclassifications:\\
    \begin{equation}
        \vect{w} \rightarrow \vect{w^\prime} = 
        \vect{w} + \alpha \Big(y - \hat{y}\Big) \vect{x} =
        \vect{w} + \alpha E(\vect{x}) \vect{x} 
        \label{eq:percepton_weight_update_1}
    \end{equation}

    A comparison of the above equations shows that, 
    implicitly, the \index{perceptron}\gls{perceptron} 
    uses the following gradient: 
    \begin{equation}
        \nabla_{\vect{w}} L = 
          \sum_{(\vect{x},y) \in \mathbb{D}} 
          (y - \hat{y}) \vect{x}
    \label{eq:percepton_smooth_gradient_1}  
    \end{equation}


    % To better understand the \gls{perceptron}, we need to ask:\\
    % What differentiable \gls{loss function} does the 
    % \gls{perceptron} (implicitly) optimize?\\

\end{frame}



%
%
%

\begin{frame}[t,allowframebreaks]{Perceptron loss function -}

    What differentiable \gls{loss function} does the 
    \gls{perceptron} (implicitly) optimize?\\
    \vspace{0.3cm}

    A \index{gradient descent}\glshyph{gradient descent} 
    update proportional to $y-\hat{y}$, 
    is naturally caused by a squared loss function like $(y-\hat{y})^2$.\\
    \vspace{0.3cm}

    So, the \index{loss function}\gls{loss function}, $L$,
    could be expressed in a form that resembles 
    \index{least-squares}\gls{least squares} 
    \index{regression}\gls{regression}:\\
    \begin{equation}
        L(\vect{w}) = 
        \sum_{(\vect{x},y) \in \mathbb{D}} 
        \Big(y - \hat{y}(\vect{x};\vect{w})\Big)^2 =
        \sum_{(\vect{x},y) \in \mathbb{D}} 
        \Bigg(y - sign\Big\{ \vect{w}^T \cdot \vect{x} \Big\}\Bigg)^2
        \label{eq:percepton_leastsquares_loss_1}  
    \end{equation}

    Unlike in \index{least-squares}\gls{least squares} 
    \index{regression}\gls{regression},
    where the gradient of the \index{loss function}\gls{loss function} 
    is a smooth and continuous function, 
    here we have to deal with the gradient of the {\em sign} function. \\

    \framebreak

    \vspace{0.3cm}

    % Unlike in \index{least-squares}\gls{least squares} 
    % \index{regression}\gls{regression},
    % where the gradient of the \index{loss function}\gls{loss function} 
    % is a smooth and continuous function, 
    % here we have to deal with the gradient of the {\em sign} function. \\
    % \vspace{0.3cm}

    The \index{loss function}\gls{loss function} of Eq.~\ref{eq:percepton_leastsquares_loss_1}
    {\bf does not provide useful gradients}:\\ 
    \begin{itemize}
        \item It jumps between values at specific points, and
        \item it is constant over large areas of its parameter space.\\
    \end{itemize}
    \vspace{0.3cm}

    Can we replace this staircase-like function, 
    with another smooth and differentiable function 
    more suitable for \index{gradient}\index{gradient descent}\gls{gradient descent}?\\
    \vspace{0.3cm}


\end{frame}

%
%
%

\begin{frame}[t]{Stochastic gradient descent for the perceptron}

    The basic training algorithm feeds 
    {\bf single training instances} $(\vect{x},y)$ into the network.
    For each instance, the algorithm:
    \begin{itemize}
        \item calculates the prediction $\hat{y}(\vect{x})$ 
        \item evaluates the prediction error $E(\vect{x})$ 
        from Eq.~\ref{eq:perceptron_prediction_error}
        ($E(\vect{x}) = y - \hat{y}$)
        \item updates the weights 
        using Eq.~\ref{eq:percepton_weight_update_1} 
        ($\vect{w} \rightarrow \vect{w^\prime} = \vect{w} + \alpha E(\vect{x}) \vect{x}$)
    \end{itemize}

    Non-zero updates are made only if we make mistakes. ($E(\vect{x}) \ne 0$.)\\
    \vspace{0.3cm}

    The algorithm cycles through all training instances in randomised order, 
    till convergence is achieved. 
    Each cycle is called an \index{epoch}\gls{epoch}.\\
    \vspace{0.3cm}
    
    Training instances can be used several times (training lasts several epochs).\\ 
    \vspace{0.3cm}

    This basic algorithm is a 
    \index{stochastic}\gls{stochastic} 
    \index{gradient}\index{gradient descent}\glshyph{gradient descent} method.\\

\end{frame}


%
%
%

\begin{frame}[t]{Mini-batch stochastic gradient descent}

    It is also possible to have a training algorithm that does not 
    update the weight after a single training example was fed into the network.\\

    Weight updated can happen a small batch of examples:
    \begin{equation}
        \vect{w} \rightarrow \vect{w^\prime} = 
          \vect{w} + \alpha \sum_{(\vect{x},y) \in \mathbb{S}} E(\vect{x}) \vect{x}
        \label{eq:percepton_weight_update_minibatch_1}
      \end{equation}

\end{frame}

%
%
%

\begin{frame}[t]{Perceptron}


\end{frame}

%
%
%

\begin{frame}[t]{Perceptron criterion}

    For a binary classification problem, 
    the number of classification errors can be expressed by the
    \index{0/1 loss function}\gls{0/1 loss function}.\\
    \vspace{0.3cm}
    For a specific training example $i$, 
    ($\vect{x}_i$,$y_i$), it is written as:
    \begin{equation}
        L_{i}^{(0/1)} = 
          \frac{1}{2} \Bigg( y_i - \hat{y_i} \Bigg)^2 =
          1 - y_i sign\Big\{ \vect{w}^T \cdot \vect{x} \Big\}.
        \label{eq:percepton_01_loss_1}  
    \end{equation}

    The \index{perceptron}\gls{perceptron} updates, 
    implicitly optimize the \index{perceptron criterion}\gls{perceptron criterion}:
    \begin{equation}
        L_{i} = 
          max\Big\{0, - y_i(\vect{w}^T \cdot \vect{x}) \Big\}.
        \label{eq:percepton_criterion_1}  
    \end{equation}

    It is obtained from the \gls{0/1 loss function}, 
    by dropping the {\em sign} function and setting all negative values to 0
    (treating all correct predictions in a uniform way).

    \noindent\rule{4cm}{0.4pt}
    {\scriptsize
        \begin{equation*}
            L_{i}^{(0/1)} = 
            \frac{1}{2} \Big( y_i - \hat{y_i} \Big)^2 =
            \frac{1}{2} \Big( \cancelto{1}{y_i^2} + \cancelto{1}{\hat{y_i}^2} - 2 y_i \hat{y_i} \Big) =          
            \frac{1}{2} \Big( 2 - 2 y_i \hat{y_i} \Big) = 
            1 - y_i sign\Big\{ \vect{w}^T \cdot \vect{x} \Big\}
        \end{equation*}     
    }
    
\end{frame}

%
%
%

\begin{frame}[t]{Perceptron}


\end{frame}




%
%
%

\begin{frame}[t]{Choice of activation function}

    In the function computed by the \index{perceptron}\gls{perceptron}: 
    \begin{equation}
        \hat{y} = sign\Big\{ \vect{w}^T \cdot \vect{x} \Big\} = sign\Big\{ \sum_{i=1}^{d} w_i x_i \Big\}
    \end{equation}        
    the {\em sign} plays the role of the \index{activation function}\gls{activation function}.\\
    \vspace{0.2cm}

    Different choices can be made, 
    simulating other \index{linear model}\glspl{linear model} in \gls{ml}.
    \begin{itemize}
        \item least-squares regression
        \item support vector machine
        \item logistic regression
    \end{itemize}
    and others.

    Simple variation to the \gls{perceptron}, can capture a wide variety of optimization methods.
    We will return to this point later in the lecture.\\

\end{frame}


%
%
%

\begin{frame}[t]{Relationship with Support Vector Machines}

    A \gls{svm} is a \gls{ml} algorithm for \index{supervised learning} 
    developed by Vladimir Vapnik and colleagues at AT\&T Bell Labs 
    \cite{Vapnik:1995svm}\cite{Wikipedia:SVM}.\\

    Like the \index{perceptron}\gls{perceptron}, \gls{svm}
    is a \index{supervised learning}\gls{supervised learning} model 
    used for \index{classification}\gls{classification} 
    and \index{regression}\gls{regression} tasks.\\

    The \index{hinge loss}\gls{hinge loss} is 
    the \index{loss function}\gls{loss function}
    used by \glspl{svm}. 
    \begin{equation}
        L_{i}^{SVM} = 
          max\Big\{0, 1 - y_i(\vect{w}^T \cdot \vect{x}) \Big\}.
        \label{eq:hinge_loss_1}  
    \end{equation}

    The \index{perceptron criterion}\gls{perceptron criterion} 
    of Eq.~\ref{eq:percepton_criterion_1}, is a \underline{shifted} \gls{hinge loss}.

    The shift {\bf does not alter the gradient}, 
    but {\bf affects which points are lossless} and do not cause an update of weights.

\end{frame}

%
%
%

\begin{frame}[t]{Relationship with Support Vector Machines}

\end{frame}
