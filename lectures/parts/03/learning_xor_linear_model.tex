%
%
%

\begin{frame}[t,allowframebreaks]{Linear models cannot learn XOR -} 

    We can choose a linear form for the model $f(\mathbf{x};\mathbf{\theta})$,
    where the parameter set $\mathbf{\theta}$ consists 
    of a set of weights $\mathbf{w}$ = $(w_1,w_2)$ and a bias $b$.        

    \begin{columns}[t]
        \begin{column}{0.50\textwidth}
            \vspace{-0.6cm}
            \begin{center}
                \begin{tikzpicture}[scale=1]
                  %\draw[help lines] (0,0) grid (6,3.7);
                  \node[ann_processing_node] (o)  at (3.0, 1.5) {$\sum$};
                  \node[ann_input_node]      (x1) at (0.0, 3.0) {$x_1$};
                  \node[ann_input_node]      (x2) at (0.0, 1.5) {$x_2$};
                  \node[ann_bias_node]       (b)  at (1.0, 0.0) {$+1$};
              
                  \drawgraphlinebigarrow (x1.east) 
                  to node[above, midway] 
                  {\small $w_1$}(o.north west) ;
              
                  \drawgraphlinebigarrow (x2.east) 
                  to node[above, midway] 
                  {\small $w_2$}(o.west) ;
              
                  \drawgraphlinebigarrow (b.east) 
                  to node[above, midway] 
                  {\small $b$}(o.south west) ;
              
                  \drawgraphlinebigarrow (o.east) 
                  to node[above,midway] 
                  {\small \color{black} $f(\mathbf{x};\mathbf{w},b)$} (6.0,1.5);              
                \end{tikzpicture}
            \end{center}        
        \end{column}
        \begin{column}{0.50\textwidth}
            \begin{equation}
                f(\mathbf{x};\mathbf{w},b) = \mathbf{x}^{T} \mathbf{w} + b
                \label{eq:learn_xor_model_1}
            \end{equation}

            As discussed previously, we can rewrite Eq.~\label{eq:learn_xor_model_1}
            in a more compact form by adding an additional input $x_0$ that always takes the value of +1 
            and considering the bias $b$ to be the weight $w_0$
            \begin{equation}
                f(\mathbf{x};\mathbf{w}) = \mathbf{x}^{T} \mathbf{w}
                \label{eq:learn_xor_model_2}
            \end{equation}
        \end{column}
    \end{columns}
      
    In principle, the loss function of Eq.~\ref{eq:learn_xor_loss_function_1} 
    could be minimised using some of the techniques we started 
    to develop in Part \prevpart.\\
    
    Owing to the simplicity of the problem, 
    it is possible to obtain a solution in closed form.

    \framebreak

    \begin{columns}[t]
        \begin{column}{0.40\textwidth}
            \vspace{-1.2cm}
            \begin{center}
                \begin{tabular}{ c c c | c }
                 $x_0$ & $x_1$ & $x_2$ & $y = x_1 \oplus x_2$ \\ 
                 \hline
                 1 & 0 & 0 & 0 \\  
                 1 & 0 & 1 & 1 \\   
                 1 & 1 & 0 & 1 \\  
                 1 & 1 & 1 & 0 \\   
                \end{tabular}
            \end{center}
        \end{column}
        \begin{column}{0.60\textwidth}
            \begin{equation*}
                \vect{X} = 
                \begin{pmatrix}
                    1 & 0 & 0 \\
                    1 & 0 & 1 \\
                    1 & 1 & 0 \\
                    1 & 1 & 1 \\
                \end{pmatrix} 
            \end{equation*}        
            \begin{equation*}
                \vect{y} = 
                \begin{pmatrix}
                    0 \\
                    1 \\
                    1 \\
                    0 \\
                \end{pmatrix} 
            \end{equation*}        
        
        \end{column}
    \end{columns}

    \begin{equation}
        \vect{w} = (\vect{X}^T \vect{X})^{-1} \vect{X}^T \vect{y}
    \end{equation}        

    \framebreak

    The matrix product $\vect{X}^T \vect{X}$ is:
    \begin{equation}
        \vect{X}^T \vect{X} = 
        \begin{pmatrix}
            1 & 1 & 1 & 1 \\
            0 & 0 & 1 & 1 \\
            0 & 1 & 0 & 1 \\
        \end{pmatrix} 
        \begin{pmatrix}
            1 & 0 & 0 \\
            1 & 0 & 1 \\
            1 & 1 & 0 \\
            1 & 1 & 1 \\
        \end{pmatrix} =
        \begin{pmatrix}
            4 & 2 & 3 \\
            2 & 2 & 2 \\
            2 & 1 & 2 \\
        \end{pmatrix} 
    \end{equation}        

    while its inverse, as it can be easily verified, is:
    \begin{equation}
        (\vect{X}^T \vect{X})^{-1} = 
        \begin{pmatrix}
            0 & -0.5 & -1 \\
            0 &  1   & -1 \\
           -1 &  0   &  2 \\
        \end{pmatrix} 
    \end{equation}        

    Therefore:
    \begin{equation*}
        \vect{w} = 
        \begin{pmatrix}
            0 & -0.5 & -1 \\
            0 &  1   & -1 \\
           -1 &  0   &  2 \\
        \end{pmatrix} 
        \begin{pmatrix}
            1 & 1 & 1 & 1 \\
            0 & 0 & 1 & 1 \\
            0 & 1 & 0 & 1 \\
        \end{pmatrix} 
        \begin{pmatrix}
            0 \\
            1 \\
            1 \\
            0 \\
        \end{pmatrix} \Rightarrow 
    \end{equation*}        

    \framebreak

    \begin{equation*}
        \vect{w} = 
        \begin{pmatrix}
            0 &  -1 &  -0.5 &  -1.5 \\
            0 &  -1 &   1   &   0   \\
           -1 &   1 &  -1   &   1   \\
        \end{pmatrix} 
        \begin{pmatrix}
            0 \\
            1 \\
            1 \\
            0 \\
        \end{pmatrix} \Rightarrow 
    \end{equation*}        

    \begin{equation}
        \vect{w} = 
        \begin{pmatrix}
            b   \\
            w_1 \\
            w_2 \\
        \end{pmatrix} =
        \begin{pmatrix}
            0.5 \\
            0 \\
            0 \\
        \end{pmatrix} 
    \end{equation}        

\end{frame}
