\renewcommand{\prevpart}{0 }
\renewcommand{\thispart}{1 }
\renewcommand{\nextpart}{2 }

\section{Introduction to Machine Learning}

% Cover page
\input{common/cover_part.tex}

% Outline
\input{common/toc_part.tex}

% Precursors
\subsection{Precursors of artificial intelligence}
\input{parts/01/ai_precursors.tex}

% History of AI
\subsection{History of artificial intelligence}
\input{parts/01/ai_history_eras.tex}
\subsubsection{Birth of AI (1952-1956)}
\input{parts/01/birth_of_ai.tex}


\subsubsection{Symbolic AI (1956-1974)}
\subsubsection{First AI winter (1974-1980)}
\subsubsection{Boom (1980-1987)}
\subsubsection{Bust: Second AI winter (1987-1993)}
\subsubsection{AI (1993-2011)}
\subsubsection{Deep learning, big data and AI (2011-present)}


%
%
%

\begin{frame}[t]{Effect of increased data}

    \begin{columns}
        \begin{column}{0.50\textwidth}
         \begin{center}
          \includegraphics[width=0.95\textwidth]{./images/dl_intro/accuracy_vs_amount_of_data_1.png}\\
          {\scriptsize \color{col:attribution} 
          Image reproduced from p.54 of \cite{Aggarwal:2018SpringerDL}}\\
         \end{center}
        \end{column}
        \begin{column}{0.50\textwidth}
        \end{column}
    \end{columns}


\end{frame}

%
%
%

\begin{frame}[t,allowframebreaks]{Increasing neural network size - }

    % Intro

    % Number of connections in various artificial neural nets as a function of time
    % and comparison with biological brains

    The human brain has $\sim$100 billion neurons and $\sim$100 trillion synapses!

    \begin{center}
        \includegraphics[width=0.95\textwidth]
          {./images/dl_intro/nnet_size_connections_vs_time_01.png}\\
        {\scriptsize \color{col:attribution} 
        Reproduced from p.22 of \cite{Goodfellow:2017DL}}\\
    \end{center}

    \framebreak

    % Number of neurons in various artificial neural nets as a function of time
    % and comparison with biological brains

    \begin{center}
        \includegraphics[width=0.95\textwidth]
           {./images/dl_intro/nnet_size_neurons_vs_time_01.png}\\
        {\scriptsize \color{col:attribution} 
        Reproduced from p.23 of \cite{Goodfellow:2017DL}}\\
    \end{center}
       {\tiny
       1. Perceptron (1958) \cite{Rosenblatt:1958p},
       2. Adaptive linear element (1960) \cite{Widrow:1960as},
       3. Neocognitron (1980) \cite{Fukushima:1980nc},
       4. Early back-propagation network (1986) \cite{Rumelhart:1986erp},
       5. Recurrent neural network for speech recognition (1991) \cite{Robinson:1991rerp},
       6. Multilayer perceptron for speech recognition (1991) \cite{Bengio:1991pma},
       7. Mean field sigmoid belief network (1996) \cite{Saul:1996mf},
       8. LeNet-5 (1998) \cite{LeCun:1998ln5},

       9. Echo state network (2004) (Jaeger and Haas, 2004)
       10. Deep belief network (2006) (Hinton et al., 2006)
       11. GPU-accelerated convolutional network (2006) (Chellapilla et al., 2006)
       12. Deep Boltzmann machine (2009) (Salakhutdinov and Hinton, 2009a)
       13. GPU-accelerated deep belief network (2009) (Raina et al., 2009)
       14. Unsupervised convolutional network (2009) (Jarrett et al., 2009)
       15. GPU-accelerated multilayer perceptron (2010) (Ciresan et al., 2010)
       16. OMP-1 network (2011) (Coates and Ng, 2011)
       
       17. Distributed autoencoder (2012) \cite{Le:2012daut}
       18. Multi-GPU convolutional network (2012) \cite{Krizhevsky:2012img},
       19. COTS HPC unsupervised convolutional network (2013) \cite{Coates:2013cots},       
       20. GoogLeNet (2014) \cite{Szegedy:2014gnet}\\
       }

    \framebreak

    % Information from recent well-known artificial neural networks

    \begin{itemize}
        \item GPT-2 had 1.5 billion parameters and around 50 billion neurons
        \item GPT-3 is estimated to have around 60-80 billion neurons
        \item GPT-4 is estimated to have around 60-80 billion neurons
    \end{itemize}

\end{frame}


\subsection{Human vs computer learning}
\input{parts/01/human_vs_computer_learning.tex}

\subsection{Different learning paradigms: Supervised, unsupervised and reinforcement learning}

\subsection{Artificial intelligence, machine learning and deep learning}

\subsection{Machine learning tasks}

\subsection{A simple practical example: Linear regression}

\subsection{Biologically inspired methods of computer learning}

\subsection{Basic architecture of neural networks}

\subsection{Fundamental concepts}
\begin{frame}[t,allowframebreaks]{Fundamental concepts - }

    \gls{relu}
\end{frame}


% From AI to DL
\input{parts/01/difference_between_ai_ml_dl.tex}

% Main points to remember
\renewcommand{\partsummarytitle}{Main points to remember }
\input{parts/01/summary.tex}

% Preview of next part
\input{parts/01/next.tex}

% Suggested reading for this part
\subsection{Suggested reading}
\input{parts/01/reading.tex}

% Optional material


